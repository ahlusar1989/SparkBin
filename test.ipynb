{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName='myAppName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(xrange(1000000))\n",
    "\n",
    "# Compute the number of primes in the RDD\n",
    "print nums.filter(isprime).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define one or more RDDs either through accessing data stored on disk (HDFS, Cassandra, HBase, Local Disk), parallelizing some collection in memory, transforming an existing RDD, or by caching or saving.\n",
    "# Invoke operations on the RDD by passing closures (functions) to each element of the RDD. Spark offers over 80 high level operators beyond Map and Reduce.\n",
    "# Use the resulting RDDs with actions (e.g. count, collect, save, etc.). Actions kick off the computing on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sc.textFile('shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The textFile method loads the complete works of Shakespeare into an RDD named text. If you inspect the RDD you \n",
    "# can see that it is a MappedRDD and that the path to the file is a relative path from the current working directory \n",
    "# (pass in a correct path to the shakespeare.txt file on your system). \n",
    "# Let's start to transform this RDD in order to compute the \"hello world\" of distributed computing: \"word count.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = text.flatMap(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[4] at RDD at PythonRDD.scala:43\n"
     ]
    }
   ],
   "source": [
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = words.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[5] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  shakespeare.txt HadoopRDD[2] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print wc.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = wc.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts.saveAsTextFile(\"wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
